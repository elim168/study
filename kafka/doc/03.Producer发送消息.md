# Producer发送消息

不管是Producer还是Consumer，Java应用需要与Kafka进行交互需要添加kafka-client依赖。

```xml
<dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-clients</artifactId>
  <version>2.3.0</version>
</dependency>
```

对于Producer，Kafka定义一个`org.apache.kafka.clients.producer.Producer`接口，它需要指定两个范型类型，分别对应消息的Key和Value的类型。它有两个实现，`org.apache.kafka.clients.producer.KafkaProducer`和`org.apache.kafka.clients.producer.MockProducer`。正常与Kafka服务器进行交互，发送消息的是KafkaProducer，MockProducer通常用于测试。KafkaProducer有几个重载的构造方法，通常我们应用的最多的就是把所有的配置信息通过一个Properties或一个Map进行传递。主要需要定义如下几个属性：

* bootstrap.servers：用来指定Kafka服务器的地址，格式是`host:port`，多个地址之间通过英文逗号分隔。如果你需要连接的不是单Kafka服务，而是一个Kafka集群，那么你不需要通过`bootstrap.servers`指定集群中所有Kafka的地址，而只需要指定其中的一些，Kafka客户端只需要通过集群中的一台机器即可获取到整个集群的地址。但是此时往往你也不能只定义其中的一个地址，因为如果你指定的那个地址的Kafka刚好挂掉了则你就连接不是整个集群了。
* acks：当Kafka服务是一个集群时，是可以针对Topic指定多个副本的，有多个副本时主节点与从节点之间需哟进行消息同步。acks用来指定Producer发送消息到了Kafka服务器后，Kafka服务器需要与多少个从节点之间进行了消息同步后才响应Producer认为此消息发送成功。如果指定`acks=0`则表示不需要任何反馈，只要消息放到了发送队列中即认为发送成功，而不管Kafka服务器是否真的收到了对应的消息。指定`acks=1`时表示只要Kafka服务器收到消息写入了本地文件即认为发送成功，就会响应给客户端，而不需要等待主节点与其它从节点之间的同步，因为主节点也算一个副本。这种情况如果主节点告诉客户端消息发送成功了，然后在消息还没有同步到任何从节点之前主节点挂了，则该消息将丢失。指定`acks=2`则表示Kafka服务器收到消息后还需要等待至少一个从节点同步消息成功后才能响应给客户端消息发送成功。如果需要消息同步到所有的从节点，则指定`acks=all`，这种情况只要集群中有一个副本节点还存活即不会出现消息丢失。
* key.serializer：指定Key需要使用的序列化实现。
* value.serializer：指定Value需要使用的序列化实现。

下面代码中我们就构造了一个KafkaProducer，然后通过其send方法进行消息发送，消息对象使用ProducerRecord表示，范型类型分别表示Key和Value的类型。需要指定Topic名称，消息Key和Value。

```java
@Test
public void test() throws Exception {
  Properties props = new Properties();
  props.put("bootstrap.servers", "localhost:9092");
  props.put("acks", "all");
  props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
  props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

  String topic = "topic1";
  Producer<String, String> producer = new KafkaProducer<>(props);
  for (int i = 0; i < 10; i++) {
    Future<RecordMetadata> future = producer.send(new ProducerRecord<String, String>(topic, "Key-" + i, "Value-" + i));
    RecordMetadata recordMetadata = future.get();
    System.out.println(recordMetadata.serializedKeySize() + "--" + recordMetadata.serializedValueSize() + "--" + recordMetadata.offset());
  }

  producer.close();
}
```

消息发送是异步的，发送后会返回一个Future对象，可以在有需要的时候通过调用其get方法获取消息的发送结果信息，用RecordMetadata对象表示，通过它可以获取消息发送的Topic、分区等信息。如果发送的消息不需要指定Key，只有Value，则可以使用ProducerRecord的只有Topic和Value的构造方法。

```java
producer.send(new ProducerRecord<String, String>(topic, "Value-" + i));
```

如果有需要你还可以指定消息发送的分区。Topic的分区是从0开始的，如果一个Topic有3个分区，那么分区的索引将是0，1，2。

```java
producer.send(new ProducerRecord<String, String>(topic, 0, "Key-" + i, "Value-" + i));
```

还可以通过timestamp指定消息发送的时间戳，毫秒表示，默认是系统当前时间。以下则指定了消息的时间戳是一天以前。

```java
long timestamp = System.currentTimeMillis() - 60 * 60 *24 * 1000;
Future<RecordMetadata> future = producer.send(new ProducerRecord<String, String>(topic, 0, timestamp, "Key-" + i, "Value-" + i));
```

ProducerRecord的参数最全的构造方法的定义如下。

```java
/**
 * Creates a record with a specified timestamp to be sent to a specified topic and partition
 * 
 * @param topic The topic the record will be appended to
 * @param partition The partition to which the record should be sent
 * @param timestamp The timestamp of the record, in milliseconds since epoch. If null, the producer will assign
 *                  the timestamp using System.currentTimeMillis().
 * @param key The key that will be included in the record
 * @param value The record contents
 * @param headers the headers that will be included in the record
 */
public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers) {
    if (topic == null)
        throw new IllegalArgumentException("Topic cannot be null.");
    if (timestamp != null && timestamp < 0)
        throw new IllegalArgumentException(
                String.format("Invalid timestamp: %d. Timestamp should always be non-negative or null.", timestamp));
    if (partition != null && partition < 0)
        throw new IllegalArgumentException(
                String.format("Invalid partition: %d. Partition number should always be non-negative or null.", partition));
    this.topic = topic;
    this.partition = partition;
    this.key = key;
    this.value = value;
    this.timestamp = timestamp;
    this.headers = new RecordHeaders(headers);
}
```

Producer使用完成后需要调用其`close()`进行关闭，以释放相应的资源。调用了`close()`后，Producer将不再接受新的发送消息的请求，而已经接受的消息将等待的它们发送完成。`close()`也有重载的指定超时时间的方法，超时时间一到将进行强制关闭。

前面介绍的Producer发送消息返回Future对象的方式， 如果你需要知道消息的发送结果，通过调用`Future.get()`，当前线程还是会进入等待。如果你希望完全的异步，可以使用重载的`send()`，通过提供一个`org.apache.kafka.clients.producer.Callback`对象，Producer进行消息发送成功（Kafka服务器正常响应）和失败后都将调用Callback对象进行回调。Callback接口的定义如下。

```java
/**
 * A callback interface that the user can implement to allow code to execute when the request is complete. This callback
 * will generally execute in the background I/O thread so it should be fast.
 */
public interface Callback {

    /**
     * A callback method the user can implement to provide asynchronous handling of request completion. This method will
     * be called when the record sent to the server has been acknowledged. When exception is not null in the callback,
     * metadata will contain the special -1 value for all fields except for topicPartition, which will be valid.
     *
     * @param metadata The metadata for the record that was sent (i.e. the partition and offset). An empty metadata
     *                 with -1 value for all fields except for topicPartition will be returned if an error occurred.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     *                  Possible thrown exceptions include:
     *
     *                  Non-Retriable exceptions (fatal, the message will never be sent):
     *
     *                  InvalidTopicException
     *                  OffsetMetadataTooLargeException
     *                  RecordBatchTooLargeException
     *                  RecordTooLargeException
     *                  UnknownServerException
     *
     *                  Retriable exceptions (transient, may be covered by increasing #.retries):
     *
     *                  CorruptRecordException
     *                  InvalidMetadataException
     *                  NotEnoughReplicasAfterAppendException
     *                  NotEnoughReplicasException
     *                  OffsetOutOfRangeException
     *                  TimeoutException
     *                  UnknownTopicOrPartitionException
     */
    void onCompletion(RecordMetadata metadata, Exception exception);
}
```

如下便是发送消息时使用`org.apache.kafka.clients.producer.Callback`的一个示例。

```java
ProducerRecord<String, String> record = new ProducerRecord<>(this.topic, "Value-" + LocalDateTime.now());
this.producer.send(record, (recordMetadata, e) -> {
  if (e != null) {
    System.out.println("消息发送失败：" + e);
  } else {
    System.out.println("消息发送成功：" + recordMetadata);
  }
});
```

（注：本文是基于Kafka2.3.0所写）