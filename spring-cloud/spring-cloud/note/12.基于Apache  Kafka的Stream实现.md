# 基于Apache Kafka的Stream实现

如果你的应用使用了Apache Kafka，你需要把它和Spring Cloud进行整合。需要在应用中添加如下依赖。

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-kafka</artifactId>
</dependency>
```

然后就是Spring Cloud Stream的标准配置了。需要在`@Configuration`类上使用`@EnableBinding`声明需要应用的Binding。

```java
@EnableBinding({Source.class, Sink.class})
@SpringBootApplication
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
    
}
```

上面代码定义需要使用的Binding是Source和Sink接口中声明的input和output两个Binding。然后可以在application.properties文件中声明这两个Binding对应的destination，它们对应于kafka的Topic。如果指定的Topic还未创建，默认会自动进行创建。

```properties
spring.cloud.stream.bindings.output.destination=test-topic
spring.cloud.stream.bindings.input.destination=test-topic
spring.cloud.stream.bindings.input.group=test-group
```

如果你的Kafka服务器不是本机或者监听端口不是默认的9092，则还需要通过`spring.cloud.stream.kafka.binder.brokers`指定Kafka的服务地址。

```properties
spring.cloud.stream.kafka.binder.brokers=localhost:9092
```

之后就是照常的使用Spring Cloud Stream的相关API进行操作了。如下是发送消息的示例。

```java
@Component
@Slf4j
public class SourceProducer {

    @Autowired
    private Source source;

    public void sendMessages(String msg) {
        Message<String> message = MessageBuilder.withPayload(msg).build();
        log.info("发送了一条消息-{}", msg);
        this.source.output().send(message);
    }

}
```

如下是监听消息的示例。

```java
@Component
@Slf4j
public class SinkConsumer {

    @StreamListener(Sink.INPUT)
    public void inputConsumer(Message<String> message) {
        String payload = message.getPayload();
        MessageHeaders headers = message.getHeaders();
        log.info("从Binding-{}收到信息-{}， headers：{}", Sink.INPUT, payload, headers);
    }
    
}

```

> 由于笔者的上一篇文章——Spring Cloud Stream基于RocketMQ的实现已经介绍了Spring Cloud Stream的一些规范，这里就不再赘述了。

（注：本文是基于Spring cloud Finchley.SR1所写）